# XTuner 微调个人小助手认知笔记

#### 1. 开发机准备
- **创建开发机**：在InternStudio中创建并配置开发机。
- **选择镜像**：使用Cuda11.7-conda镜像。
- **资源配置**：选择10% A100 * 1的资源选项。
- **进入终端**：通过点击Terminal进入终端界面开始操作。

#### 2. 快速上手
**环境安装**：安装XTuner及其依赖。

- **克隆环境**：在InternStudio平台克隆pytorch环境或在其他平台创建新环境。

- **激活环境**：使用`conda activate`命令激活XTuner环境。

- **拉取源码**：从GitHub或Gitee克隆XTuner的0.1.17版本源码。

- **安装XTuner**：通过`pip install -e '.[all]'`安装XTuner。

  ```python
  # 如果你是在 InternStudio 平台，则从本地 clone 一个已有 pytorch 的环境：
  # pytorch    2.0.1   py3.10_cuda11.7_cudnn8.5.0_0
  studio-conda xtuner0.1.17
  # 如果你是在其他平台：
  # conda create --name xtuner0.1.17 python=3.10 -y
  # 激活环境
  conda activate xtuner0.1.17
  # 进入家目录 （~的意思是 “当前用户的home路径”）
  cd ~
  # 创建版本文件夹并进入，以跟随本教程
  mkdir -p /root/xtuner0117 && cd /root/xtuner0117
  # 拉取 0.1.17 的版本源码
  git clone -b v0.1.17  https://github.com/InternLM/xtuner
  # 无法访问github的用户请从 gitee 拉取:
  # git clone -b v0.1.15 https://gitee.com/Internlm/xtuner
  # 进入源码目录
  cd /root/xtuner0117/xtuner
  # 从源码安装 XTuner
  pip install -e '.[all]'
  ```

**前期准备**：明确微调目标，准备数据集和硬件资源。

- 数据集准备**：
  - 创建文件夹存放训练文件。
  - 编写`generate_data.py`脚本来生成个性化数据集。
  - 运行脚本生成`personal_assistant.json`数据集文件。
- **模型准备**：
  - 选择InterLM2-Chat-1.8B模型。
  - 复制模型文件到本地目录或创建符号链接。
- **配置文件选择**：
  - 使用`xtuner list-cfg`命令列出可用的配置文件。
  - 选择与微调目标匹配的配置文件并复制到本地目录。

#### 3. 配置文件修改
- **修改配置文件**：根据个性化需求调整配置文件中的路径、超参数等设置。

#### 4. 模型训练
- **常规训练**：使用`xtuner train`命令开始训练，可通过`--work-dir`指定保存路径。
- **使用deepspeed加速**：通过指定`--deepspeed`选项来加速训练。

#### 5. 模型转换、整合、测试及部署
- **模型转换**：将训练好的模型权重转换为Huggingface格式。
- **模型整合**：将微调的adapter层与原模型组合，形成完整的模型。
- **对话测试**：使用XTuner提供的对话测试功能进行模型测试。
- **Web demo部署**：使用streamlit部署模型，进行交互式测试。

# 注意事项
- **数据集质量**：高质量的数据集是微调成功的关键。
- **过拟合问题**：注意模型过拟合现象，适时保存和选择最优权重。
- **模型续训**：了解模型续训的方法，以便在训练中断后能够恢复。
- **模型量化部署**：微调并测试满意的模型可以进一步进行量化部署。

