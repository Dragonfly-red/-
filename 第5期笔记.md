# LMDeploy 量化部署 LLM&

1. 大模型部署背景

   在软件工程中，部署通常子的是将开发完毕的软件投入使用的过程

   在人工智能领域，模型部署是实现深度学习算法落地应用的关键步骤，简单来说，模型部署教师将训练好的深度学习模型在特定环境中运行的过程

2. 模型剪枝

   通过一处模型中不必要的连接或者参数，降低冗余，同时保证模型性能最低下降，按照是否爆出网络整体结构，可分为非结构化剪枝和结构化剪枝

   **非结构化剪枝：**移除个别参数，不考虑整体网络结构，通过这种方法将地域阈值的参数置零的方式对个别权重或者神经元进行处理

   **结构化剪枝：**根据预定义规则一处连接或者分层结构，同时保持整体网络结构。这种方法一次性的针对整组权重，优势在于降低模型复杂性和内存使用，同时保持整体的LLM结构完整

3. 知识蒸馏

   核心思想是通过引导轻量化的学生模型“模仿“性能更好，结构更复杂的教师模型，在不改变学生模型结构的情况下提高其性能

4. 量化

   将传统标识方法中的浮点数转换为整数或其他离散形式，以减轻深度学习模型的存储和计算负担

# LMDeploy简介&环境部署

LMDeploy 由 MMDeploy 和 MMRazor 团队联合开发是涵盖了 LLM 任务的全套轻量化、部署和服务解决方案。核心功能包括高效推理、可靠量化、便捷服务和有状态推理。

- 高效的推理:LMDeploy开发了Continuous Batch，Blocked K/ Cache，动态拆分和融合，张量并行，高效的计算kernel等重要特性。InternLM2推理性能是vLLM的 1.8 倍。

- 可靠的量化:LMDeploy支持权重量化和k/量化。4bit模型推理效率是FP16下的2.4倍。量化模型的可靠性已通过OpenCompass评测得到充分验证。
- 便捷的服务:通过请求分发服务，LMDeploy 支持多模型在多机、多卡上的推理服务。
- 有状态推理:通过缓存多轮对话过程中Attention的k/，记住对话历史，从而避免重复处理历史会话。显著提升长文本多轮对话场景中的效率。

1. 创建环境

   ```python
   studio-conda -t lmdeploy -o pytorch-2.1.2
   ```

2. 安装LMDeploy

   ```python
   conda activate lmdeploy
   pip install lmdeploy[all]==0.3.0
   ```

   